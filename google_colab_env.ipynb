{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcdff4d",
   "metadata": {},
   "source": [
    "# ML Reproducibility Challenge 2022\n",
    "## TS2Vec: Towards Universal Representation of Time Series\n",
    "\n",
    "The original paper can be found [here](https://arxiv.org/pdf/2106.10466.pdf)\n",
    "\n",
    "Original code implementation can be found on [github/](https://github.com/yuezhihan/ts2vec.git)\n",
    "\n",
    "ML Reproducibility Challenge 2022 [homepage](https://paperswithcode.com/rc2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yuezhihan/ts2vec.git # clones TS2Vec repo\n",
    "!python3 --version # want: 3.8.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbae7a8",
   "metadata": {},
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "Our Google Colab environment came with a python3.7 kernel installed. TS2Vec requires python3.8, so we had to preparing the environment accordingly. \n",
    "\n",
    "Installs python3.8 and library requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa137c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install python3.8\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2\n",
    "!python3 --version # want: 3.8.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating python version breaks pip -- get pip\n",
    "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "!python3 get-pip.py\n",
    "\n",
    "# install requirements - their requirements.txt file didn't work\n",
    "!pip3 install torch\n",
    "!pip3 install numpy\n",
    "!pip3 install scikit_learn==0.24.2\n",
    "!pip3 install bottleneck\n",
    "!pip3 install pandas\n",
    "!pip3 install statsmodels\n",
    "!pip3 install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf53000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ts2vec # change directory to cloned repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8dfa4",
   "metadata": {},
   "source": [
    "## Reproducing paper results\n",
    "\n",
    "Performance metrics presented in the results section can be reproduced by running TS2Vec packaged functions provided in the $\\texttt{scripts/}$ folder.\n",
    "\n",
    "Datasets must be manually prepared according to instructions in the TS2Vec $\\texttt{README.md}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91fee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample reproduction line\n",
    "# !python3 -u train.py ETTh1 forecast_univar --loader forecast_csv_univar --repr-dims 320 --max-threads 8 --seed 42 --eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6945d",
   "metadata": {},
   "source": [
    "## Running TS2Vec on a new dataset\n",
    "\n",
    "The dataset is of hydrologic data (water depth) measured over the course of 18 months. Data provided by Open-storm -- [link to grafana dashboard](http://ec2-3-142-80-107.us-east-2.compute.amazonaws.com:3000/d/RgZSCbz7zz/honey-creek-at-dexter?orgId=1&from=now-30d&to=now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run TS2Vec forecasting on hydrologic dataset with all default parameters\n",
    "# !python3 train.py ARB048_format ARB_run --loader forecast_csv --eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c5add",
   "metadata": {},
   "source": [
    "## Visualizing Forecasting Performance\n",
    "\n",
    "To visualize the forecast predicitons, we had to modify several functions from the $\\texttt{tasks/}$ folder. This step requires already having trained a model on the hydrologic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93480b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from TS2Vec \n",
    "from ts2vec import TS2Vec\n",
    "from tasks.forecasting import generate_pred_samples, cal_metrics\n",
    "\n",
    "# import libraries\n",
    "import datautils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.linear_model import Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model \n",
    "model = TS2Vec(input_dims=9)\n",
    "file_path = 'training/ARB048_format__ARB_run_20221024_233357/model.pkl' # modify based on output of training step\n",
    "model.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the datasets\n",
    "data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols = datautils.load_forecast_csv('ARB048_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6666e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time-encoded input data\n",
    "fig, ax = plt.subplots(figsize=(12,6),nrows=2, sharex=True)\n",
    "x = np.linspace(0,data.shape[1]-1, data.shape[1])\n",
    "ax[0].scatter(x, data[0,:,-1], marker='.')\n",
    "ax[0].set_title(\"Original Hydrologic Timeseries\")\n",
    "for i in range(9):\n",
    "    ax[1].scatter(x, data[0,:,i], marker='.', alpha=0.5, label=i)\n",
    "ax[1].set_title(\"Expanded dimensional data - Datetime elements separated out as features\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cffe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(train_features, train_y, valid_features, valid_y, MAX_SAMPLES=100000):\n",
    "    # If the training set is too large, subsample MAX_SAMPLES examples\n",
    "    if train_features.shape[0] > MAX_SAMPLES:\n",
    "        split = train_test_split(\n",
    "            train_features, train_y,\n",
    "            train_size=MAX_SAMPLES, random_state=0\n",
    "        )\n",
    "        train_features = split[0]\n",
    "        train_y = split[2]\n",
    "    if valid_features.shape[0] > MAX_SAMPLES:\n",
    "        split = train_test_split(\n",
    "            valid_features, valid_y,\n",
    "            train_size=MAX_SAMPLES, random_state=0\n",
    "        )\n",
    "        valid_features = split[0]\n",
    "        valid_y = split[2]\n",
    "    \n",
    "    alphas = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "    valid_results = []\n",
    "    for alpha in alphas:\n",
    "        lr = Ridge(alpha=alpha).fit(train_features, train_y)\n",
    "        valid_pred = lr.predict(valid_features)\n",
    "        score = np.sqrt(((valid_pred - valid_y) ** 2).mean()) + np.abs(valid_pred - valid_y).mean()\n",
    "        valid_results.append(score)\n",
    "    best_alpha = alphas[np.argmin(valid_results)]\n",
    "    \n",
    "    lr = Ridge(alpha=best_alpha)\n",
    "    lr.fit(train_features, train_y)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa82355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_eval_forecasting(model, data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols):\n",
    "    padding = 200\n",
    "    \n",
    "    t = time.time()\n",
    "    all_repr = model.encode(\n",
    "        data,\n",
    "        casual=True,\n",
    "        sliding_length=1,\n",
    "        sliding_padding=padding,\n",
    "        batch_size=256\n",
    "    )\n",
    "    ts2vec_infer_time = time.time() - t\n",
    "    \n",
    "    train_repr = all_repr[:, train_slice]\n",
    "    valid_repr = all_repr[:, valid_slice]\n",
    "    test_repr = all_repr[:, test_slice]\n",
    "    \n",
    "    train_data = data[:, train_slice, n_covariate_cols:]\n",
    "    valid_data = data[:, valid_slice, n_covariate_cols:]\n",
    "    test_data = data[:, test_slice, n_covariate_cols:]\n",
    "    \n",
    "    ours_result = {}\n",
    "    lr_train_time = {}\n",
    "    lr_infer_time = {}\n",
    "    out_log = {}\n",
    "    collect_test_pred = {}\n",
    "    collect_test_labels = {}\n",
    "    for pred_len in pred_lens:\n",
    "        train_features, train_labels = generate_pred_samples(train_repr, train_data, pred_len, drop=padding)\n",
    "        valid_features, valid_labels = generate_pred_samples(valid_repr, valid_data, pred_len)\n",
    "        test_features, test_labels = generate_pred_samples(test_repr, test_data, pred_len)\n",
    "        \n",
    "        t = time.time()\n",
    "        lr = fit_ridge(train_features, train_labels, valid_features, valid_labels)\n",
    "        lr_train_time[pred_len] = time.time() - t\n",
    "        \n",
    "        t = time.time()\n",
    "        test_pred = lr.predict(test_features)\n",
    "        lr_infer_time[pred_len] = time.time() - t\n",
    "\n",
    "        ori_shape = test_data.shape[0], -1, pred_len, test_data.shape[2]\n",
    "        test_pred = test_pred.reshape(ori_shape)\n",
    "        test_labels = test_labels.reshape(ori_shape)\n",
    "        collect_test_pred[pred_len] = test_pred\n",
    "        collect_test_labels[pred_len] = test_labels\n",
    "        \n",
    "    #     test_pred_inv = scaler.inverse_transform(test_pred)\n",
    "    #     test_labels_inv = scaler.inverse_transform(test_labels)\n",
    "    #     if test_data.shape[0] > 1:\n",
    "    #         test_pred_inv = scaler.inverse_transform(test_pred.swapaxes(0, 3)).swapaxes(0, 3)\n",
    "    #         test_labels_inv = scaler.inverse_transform(test_labels.swapaxes(0, 3)).swapaxes(0, 3)\n",
    "    #     else:\n",
    "    #         test_pred_inv = scaler.inverse_transform(test_pred)\n",
    "    #         test_labels_inv = scaler.inverse_transform(test_labels)\n",
    "            \n",
    "    #     out_log[pred_len] = {\n",
    "    #         'norm': test_pred,\n",
    "    #         'raw': test_pred_inv,\n",
    "    #         'norm_gt': test_labels,\n",
    "    #         'raw_gt': test_labels_inv\n",
    "    #     }\n",
    "    #     ours_result[pred_len] = {\n",
    "    #         'norm': cal_metrics(test_pred, test_labels),\n",
    "    #         'raw': cal_metrics(test_pred_inv, test_labels_inv)\n",
    "    #     }\n",
    "        \n",
    "    # eval_res = {\n",
    "    #     'ours': ours_result,\n",
    "    #     'ts2vec_infer_time': ts2vec_infer_time,\n",
    "    #     'lr_train_time': lr_train_time,\n",
    "    #     'lr_infer_time': lr_infer_time\n",
    "    # }\n",
    "    # return out_log, eval_res, \n",
    "    return collect_test_pred, collect_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44502537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return test predictions and labels used to calculate the performance metrics\n",
    "test_pred, test_labels = mod_eval_forecasting(model, data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_pred[24]\n",
    "labels= test_labels[24]\n",
    "cal_metrics(predictions, labels) #confirm performance metrics are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction forecast for pred_length=24\n",
    "fig, ax = plt.subplots(figsize=(12,6),nrows=2, sharex=True, sharey=True)\n",
    "ax[0].plot(labels[0,:,0,0], labels[0,:,0,1])\n",
    "ax[1].scatter(predictions[0,:,0,0], predictions[0,:,0,1], marker='.', c='orange')\n",
    "ax[0].set_title('Scaled time-masked hydrologic data')\n",
    "ax[1].set_title('Prediction, pred_length=24')\n",
    "predictions[0,:,0,0] - labels[0,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac719deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all pre_length forecast predictions overlaid\n",
    "pred24= test_pred[24]\n",
    "pred48= test_pred[48]\n",
    "pred96= test_pred[96]\n",
    "pred288= test_pred[288]\n",
    "pred672= test_pred[672]\n",
    "fig, ax = plt.subplots(figsize=(12,6),nrows=2, sharex=True, sharey=True)\n",
    "ax[0].plot(labels[0,:,0,0], labels[0,:,0,1])\n",
    "ax[1].scatter(pred672[0,:,0,0], pred672[0,:,0,1], marker='.', alpha=0.3, label='672')\n",
    "ax[1].scatter(pred288[0,:,0,0], pred288[0,:,0,1], marker='.', alpha=0.3, label='288')\n",
    "ax[1].scatter(pred96[0,:,0,0], pred96[0,:,0,1], marker='.', alpha=0.3, label='96')\n",
    "ax[1].scatter(pred48[0,:,0,0], pred48[0,:,0,1], marker='.', alpha=0.3, label='48')\n",
    "ax[1].scatter(pred24[0,:,0,0], pred24[0,:,0,1], marker='.', alpha=0.3, label='24')\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Scaled time-masked hydrologic data')\n",
    "ax[1].set_title('Prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
